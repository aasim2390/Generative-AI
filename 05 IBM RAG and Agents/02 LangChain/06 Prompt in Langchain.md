## ğŸ“˜ Notes on Prompts in LangChain

Prompts are the instructions we give to a language model (like GPT).

The way you design them decides what kind of answer you get.

ğŸ‘‰ One-liner: â€œPrompt = Recipe. Good recipe â†’ Good dish.â€

---

### ğŸ”¹ What is a Prompt?

A prompt is text (or other input) that tells the model what to do.

It controls tone, format, and accuracy of the modelâ€™s output.

**Example:**
```python
"Explain gravity in 2 sentences."
```

### ğŸ”¹ Input Types

Text-based â†’ normal strings.

Multimodal â†’ some models also accept images, audio, or video, but LangChain mostly works with text.

ğŸ‘‰ One-liner: â€œPrompts talk to the brain of AIâ€”words are the language.â€

| Type    | Meaning                 | Example                           |
| ------- | ----------------------- | --------------------------------- |
| Static  | Fixed, no variables     | `"Tell me a joke about cats."`    |
| Dynamic | Template with variables | `"Tell me a joke about {animal}"` |

```python
from langchain_core.prompts import PromptTemplate

tpl = PromptTemplate.from_template(
    "Explain {topic} in {length} sentences."
)
```

ğŸ‘‰ One-liner: â€œStatic is frozen, Dynamic adapts.â€

---

### ğŸ”¹ Why use PromptTemplate?

- Validation â†’ ensures required variables are filled.

- Reuse â†’ save & load prompts easily.

```python
template.save("mytemplate.json")
load_prompt("mytemplate.json")
```

- Ecosystem fit â†’ works smoothly inside LangChain chains.

ğŸ‘‰ One-liner: â€œPromptTemplate = Reusable Lego block for prompts.â€

---

### ğŸ”¹ ChatPromptTemplate

Lets you build multi-message conversations with roles (system, human, AI).

Good for making chatbots.

Example:

```python
from langchain_core.prompts import ChatPromptTemplate

chat_tpl = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "{question}")
])

chat_tpl.format_messages(question="What is 2+2?")

```

ğŸ‘‰ One-liner: â€œChatPrompt = Multi-turn conversation recipe.â€

---

### ğŸ”¹ MessagesPlaceholder

Special slot to insert conversation history into a prompt.

Keeps context alive.

Example:
```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

chat_tpl = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder("history"),
    ("human", "{question}")
])

chat_tpl.invoke({
    "history": [("human", "Hi"), ("ai", "Hello!")],
    "question": "How are you?"
})

```

ğŸ‘‰ One-liner: â€œPlaceholder = Memory slot for past chats.â€
---

### ğŸ”¹ Simple Chatbot Example

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

chat_tpl = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Type 'quit' to exit."),
    MessagesPlaceholder("history"),
    ("human", "{user_input}")
])

history = []
while True:
    user_input = input("You: ")
    if user_input.lower() in ("quit", "exit"):
        print("Bye!")
        break
    history.append(("human", user_input))
    resp = llm.invoke(chat_tpl.invoke({"history": history, "user_input": user_input}))
    print("Bot:", resp.content)
    history.append(("ai", resp.content))
```

ğŸ‘‰ One-liner: â€œChatbot = ChatPrompt + Memory + Loop.â€

---

### ğŸ”¹ Advanced Prompting
1. Dynamic Few-Shot

Instead of fixed examples, pick the most relevant examples at runtime.

Saves tokens and improves relevance.

Example:

```python
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS

examples = [
    {"question": "What is GPT?", "answer": "GPT is a large language model."},
    {"question": "Define Transformer.", "answer": "Transformer is a neural model for sequence modeling."}
]

# Build simple example store
tpl = PromptTemplate.from_template("Q: {question}\nA: {answer}")
docs = [tpl.format(**ex) for ex in examples]

emb = OpenAIEmbeddings()
db = FAISS.from_texts(docs, emb)

query = "Explain what a large language model is?"
relevant = db.similarity_search(query, k=1)

few_shot = "\n".join([d.page_content for d in relevant])
prompt = f"{few_shot}\n\nQ: {query}\nA:"
llm = ChatOpenAI()
print(llm.invoke(prompt))

```

ğŸ‘‰ One-liner: â€œFew-shot = Learn by example, dynamic = right example at the right time.â€

---

### 2. Meta / Adaptive Prompting

Use a model to improve or rewrite prompts.

Great for refining unclear instructions.

Example:
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

# Meta-prompt
initial = "Explain photosynthesis simply."

meta_prompt = f"""
You are a prompt engineer. Rewrite this prompt to be clearer:
'{initial}'
"""
refined = llm.invoke(meta_prompt).content
print("Refined Prompt:", refined)

# Use refined prompt
print("Answer:", llm.invoke(refined).content)

```

ğŸ‘‰ One-liner: â€œMeta-prompting = AI helps you write better prompts.â€

---

### âœ… Quick Recap

Prompt â†’ Instructions (the recipe).

Static vs Dynamic â†’ Fixed vs flexible.

PromptTemplate â†’ Reusable validated recipes.

ChatPromptTemplate â†’ Multi-turn chat.

MessagesPlaceholder â†’ Keeps history.

Few-Shot â†’ Learn by examples.

Dynamic Few-Shot â†’ Pick examples smartly.

Meta/Adaptive â†’ AI refines prompts.

Chatbot â†’ Glue them together.

ğŸ‘‰ Final One-liner: â€œPrompts are recipes, templates are cookbooks, chat prompts serve meals, few-shot learns from past dishes, meta-prompting makes the recipe better.â€
