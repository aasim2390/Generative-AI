# -*- coding: utf-8 -*-
"""Product_Launch_CrewAI.ipynb

Automatically generated by Colab.
"""

# üöÄ LaunchMaster ‚Äì Product Launch Orchestrator  

LaunchMaster is an **AI-powered orchestrator** (built on CrewAI) designed to manage every stage of a product launch.  
It leverages specialized agents, web tools, and human-in-the-loop approvals to ensure launches are **well-researched, engaging, and impactful**.  

---

## üìå Requirements  

1. **Environment Setup**  
   - Python 3.9+  
   - Dependencies: `crewai`, `crewai-tools`, `pydantic` 

2. **API Keys**  
   - `OPENAI_API_KEY` ‚Üí for LLM calls  
   - `SERPER_API_KEY` ‚Üí for search tool  

3. **Artifacts Directory**  
   - Outputs saved under `./outputs/<timestamp>/` as Markdown + JSON  

---

## üîë Core Concepts  

- **CrewAI Framework**: Manages multi-agent workflows with defined roles and tasks  
- **External Tools**:  
  - `SerperDevTool` ‚Üí web search  
  - `ScrapeWebsiteTool` ‚Üí website scraping  
- **Custom Tools**:  
  - `SaveMarkdownTool` ‚Üí save generated content locally  
  - `PauseForApprovalTool` ‚Üí human approval before finalizing  

---

## ‚öôÔ∏è Possibilities (Workflow Steps)  

1. **Market Research**  
   - Identify target demographics, competitors, and trends  
   - Summarize insights in Markdown reports  

2. **Content Creation**  
   - Generate persuasive copy for landing pages, ads, emails, social media, and press releases  

3. **Influencer & Media Outreach**  
   - Create prioritized contact lists  
   - Draft personalized outreach templates  

4. **Human Review (Optional)**  
   - Pause execution to allow user approval before saving outputs  

5. **Final Delivery**  
   - Save all deliverables as structured Markdown + JSON files  

---
"""

!pip install crewai crewai-tools pydantic rich python-dotenv openai

from crewai_tools import ScrapeWebsiteTool, SerperDevTool
from pydantic import BaseModel

"""### The *ScrapeWebsiteTool* for AI

Imagine you're an AI assistant and you need to get the latest news from a website. You can't just "look" at the internet like a human. You need a special tool to do it. That's where the ScrapeWebsiteTool comes in. It's like a digital hand that can grab information from websites for an AI to read and understand. This is called "web scraping."

To make this work, the AI needs another tool called SerperDevTool, which is like a super-fast search engine. You have to get a special key from the serper.dev website to use it, just like you need a key to get into a locked building.  This key lets the AI use the search tool to find the right website before it scrapes the information.

### What's a *pydantic.BaseModel*?

Think of a pydantic.BaseModel as a blueprint for data. Let's say you're building a character for a video game. The blueprint would say things like "this character must have a name (which is text)," "this character must have a health score (which is a number)," and "this character's special ability must be an item from a list."

The BaseModel does the same thing for data. It's a way to tell the computer exactly what kind of information to expect. If the AI is trying to scrape a website, the BaseModel makes sure the information it gets back is in the right format, like a title, a date, and the article's text. This prevents errors and makes the whole process run smoothly.

### The *SerperDevTool* is a tool that allows an AI to perform a Google search.

Instead of seeing a messy web page, the AI gets the search results in a clean, organized format, making it easy to understand and use the information. This is especially useful for AI agents that need to access real-time, up-to-date information from the internet for tasks like research or content creation.

**SerperDevTool (The Search Engine)**: This is the first step. The SerperDevTool is like a smart search engine for AI. It takes a user's question or command and performs a search on the internet using the Serper API. Instead of getting a visual webpage, the AI receives a structured list of results, including titles, links, and short descriptions. This organized data makes it easy for the AI to find the most relevant sources.

**ScrapeWebsiteTool (The Data Extractor):** Once the SerperDevTool finds the right websites, the ScrapeWebsiteTool takes over. Its job is to "scrape" or extract the actual content from a specific webpage. If the search tool finds a promising news article, the scraping tool will go to that link and pull out the text of the article itself. This allows the AI to read the full content, not just the search snippet.

Together, they allow an AI to first find information on the web and then collect the detailed data it needs from those sources.
"""

from google.colab import userdata
SERPER_API_KEY = userdata.get('SERPER_API_KEY')
OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')

import os

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["SERPER_API_KEY"] = SERPER_API_KEY

from crewai import Agent, Task, Process,Crew

search_tool = SerperDevTool()
scrape_tool = ScrapeWebsiteTool()

market_researcher = Agent(
    role="Market Researcher",
    goal="Conduct thorough market research to identify target demographics and competitors.",
    model="gpt-3.5-turbo",
    max_tokens=1500,
    max_words=2500,
    tools=[search_tool, scrape_tool],
    verbose=True,
    backstory=(
        "Analytical and detail-oriented, you excel at gathering insights about the market, "
        "analyzing competitors, and identifying the best strategies to target the desired audience."
    )
)

content_creator = Agent(
    role='Content Creator',
    goal="Develop engaging content for the product launch, including blogs, social media posts, and videos.",
    model="gpt-3.5-turbo",
    max_tokens=1500,
    max_words=2500,
    tools=[search_tool, scrape_tool],
    verbose=True,
    backstory=(
        "Creative and persuasive, you craft content that resonates with the audience, "
        "driving engagement and excitement for the product launch."
    )
)

pr_outreach_specialist = Agent(
    role="PR and Outreach Specialist",
    goal="Reach out to influencers, media outlets, and key opinion leaders to promote the product launch.",
    model="gpt-3.5-turbo",
    max_tokens=1500,
    max_words=2500,
    tools=[search_tool, scrape_tool],
    verbose=True,
    backstory=(
        "With strong networking skills, you connect with influencers and media outlets to ensure "
        "the product launch gains maximum visibility and coverage."
    )
)

"""**Market Researcher**: Gathers and analyzes data to find the right customers and competitors.

**Content Creator:** Writes blogs and social media posts to build excitement.

**PR and Outreach Specialist:** Reaches out to influencers and media to get the word out.
"""

class MarketResearchData(BaseModel):
    target_demographics: str
    competitor_analysis: str
    key_findings: str

"""The ***MarketResearchData*** class is a structured blueprint for storing market research findings. It ensures that any data collected by the Market Researcher agent is saved in a consistent format.

This class has three main parts:

- **target_demographics**: A text field for describing the ideal customer.
- **competitor_analysis**: A text field for detailing what competitors are doing.
- **key_findings**: A text field for highlighting the most important conclusions from the research.


This organized approach guarantees that all key information is captured and easy to access.
"""

market_research_task = Task(
    description="Conduct market research for the {product_name} launch, focusing on target demographics and competitors.",
    expected_output="A detailed report on market research findings, including target demographics and competitor analysis.",
    human_input=True,
    output_json=MarketResearchData,
    output_file="market_research.json",
    agent=market_researcher
)

"""This task is designed to create a structured and accurate report of market research findings. It ensures consistency by formatting the output according to the ***MarketResearchData*** model.

### Here's how it works:

**Structured Output**: The results are saved in a specific format, ensuring all key information is present and organized.

**Human Feedback**: A human_input parameter is included, allowing a person to review and adjust the results before they're finalized. This is crucial for accuracy and quality control.

**JSON File:** The final output is saved as a JSON file named ‚Äúmarket_research.json‚Äù. This makes it easy to share the data with other teams or systems, ensuring a smooth workflow for the product launch.
"""

content_creation_task = Task(
   description="Create content for the {product_name} launch, including blog posts, social media updates, and promotional videos.",
    expected_output="A collection of content pieces ready for publication.",
    human_input=True,
    async_execution=False,  # Change to synchronous
    output_file="content_plan.txt",
    agent=content_creator
)

"""**async_execution=False** means the task will be run synchronously. This tells the AI system to complete this specific task entirely before moving on to the next one. It ensures that the content_creation_task must be finished and its output (content_plan.txt) is ready before any subsequent tasks that might depend on it can start.


This is the opposite of async_execution=True, which would allow the task to run in the background while other tasks are initiated.

The task tells the Content Creator agent to make content for a product launch. Its main goal is to create a set of ready-to-publish materials to promote the new product.

### Key Features:
**Content Creation:** The agent is responsible for writing things like blog posts, social media updates, and other promotional content.

**Human Feedback:** A parameter is included for human input, allowing someone to review and approve the content before it's finalized. This ensures the tone and message are just right.

**Final Output**: The finished content is saved as a single text file named "content_plan.txt". This makes it easy to use and share with others involved in the launch.
"""

pr_outreach_task = Task(
    description="Contact influencers, media outlets, and key opinion leaders to promote the {product_name} launch.",
    expected_output="A report on outreach efforts, including responses from influencers and media coverage.",
    async_execution=False,  # Change to synchronous
    output_file="outreach_report.md",
    agent=pr_outreach_specialist
)

"""The agent's job is to contact key people and organizations to secure media coverage and visibility for the new product. This includes communicating with:

**Influencers:** People who have a large following and can promote the product to their audience.

**Media Outlets:** News publications, blogs, and other media that can feature the product.

**Key Opinion Leaders (KOLs)**: Respected experts in the industry who can provide a trusted endorsement.

The final result of this task is a detailed outreach_report.md file. This report summarizes all the outreach activities and includes feedback and responses from those who were contacted, giving a clear overview of the campaign's success.
"""

product_launch_crew = Crew(
    agents=[market_researcher, content_creator, pr_outreach_specialist],
    tasks=[market_research_task, content_creation_task, pr_outreach_task],  # Ensure only one async task is at the end
    verbose=True
)

"""This crew consists of the Market Researcher, Content Creator, and PR and Outreach Specialist agents."""

launch_details = {
    'product_name': "Copper Water Bottle",
    'product_description': "1.5 Liter Copper Water Bottle for good health, Only in India.",
    'launch_date': "2025-08-20",
    'target_market': "Health and fitness",
    'budget': 1200
}

"""The ***launch_details*** dictionary holds all the important information for a product launch.

When this dictionary is given to the product_launch_crew, the AI agents (the Market Researcher, Content Creator, and PR Specialist) use these details as instructions to start their tasks, making sure their work is perfectly suited for this specific product launch.
"""

result = product_launch_crew.kickoff(inputs=launch_details)

"""
***Key Aspects of the Process***

**Data Sourcing:** The program gathers a vast amount of data by accessing the Internet. This initial step is responsible for the extensive, detailed output.

**Data Transformation:** The next step involves refining this information to a much shorter, more manageable format. This process likely involves summarization, key phrase extraction, and eliminating redundant or less important details.

**Purpose:** The main purpose is to make the extensive data more accessible and useful by presenting the most critical insights without the overwhelming volume. This is a common practice in data analysis and reporting.
"""

import json
from pprint import pprint
from IPython.display import Markdown  # Add this line to import the Markdown function
# Display the generated market_research.json file
with open('market_research.json') as f:
   data = json.load(f)
pprint(data)
# Display the generated content_plan.txt file
with open('content_plan.txt') as f:
    content = f.read()
print(content)
# Display the generated outreach_report.md file
Markdown("outreach_report.md")

"""The text describes a process for displaying the final outputs from three AI agents.

Here's a breakdown of the steps:

**Market Research Data:** The market_research.json file, which contains structured data from the Market Researcher agent, is loaded and printed in a neat, easy-to-read format using the json and pprint modules.

**Content Plan:** The content_plan.txt file, containing the content created by the Content Creator agent, is opened, read, and displayed directly.

**Outreach Report:** The outreach_report.md file, which contains the PR and Outreach Specialist's report in Markdown format, is rendered to be viewed properly.

This sequence ensures all the key outputs from the agents‚Äîmarket data, content, and outreach results‚Äîare presented in a clear, user-friendly way for review.
"""

